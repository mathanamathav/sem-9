# -*- coding: utf-8 -*-
"""IR-CA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlkbTeQMBJSi7v22zlhK73d70U1TAHue
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD
import matplotlib.pyplot as plt
import pandas as pd
import math
import string
from itertools import groupby

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("omw-1.4")


class TextAnalyzer:
    def __init__(self, keywords_file, documents_file):
        self.keywords = open(keywords_file).readlines()
        self.keywords = [x.strip() for x in self.keywords]

        self.documents = open(documents_file, encoding="utf-8-sig").read().split("\n\n")
        self.documents = [
            x.replace("\n", " ").replace("...", "").replace("ï¿½", "").replace(".  ", ".")
            for x in self.documents
        ]

        self.process_docs = [self.preprocess(x).lower() for x in self.documents]

        self.vectorizer = TfidfVectorizer(vocabulary=self.keywords)
        self.X = self.vectorizer.fit_transform(self.process_docs)

        self.countvect = CountVectorizer(
            min_df=5,
            max_df=0.9,
            stop_words="english",
            lowercase=True,
            token_pattern="[a-zA-Z\-][a-zA-Z\-]{2,}",
        )
        self.count_docs = self.countvect.fit_transform(self.process_docs)

        self.NUM_TOPICS = 10
        self.lda = LatentDirichletAllocation(
            n_components=self.NUM_TOPICS,
            max_iter=10,
            learning_method="online",
            verbose=True,
        )
        self.data_lda = self.lda.fit_transform(self.count_docs)

    def remove_stop_words(self, text):
        stop_words = set(stopwords.words("english"))
        words = text.split()
        words = [word for word in words if word not in stop_words]
        return " ".join(words)

    def remove_punctuation(self, text):
        translator = str.maketrans("", "", string.punctuation)
        text = text.translate(translator)
        return text

    def lowercase(self, text):
        return text.lower()

    def preprocess(self, text):
        text = self.lowercase(text)
        text = self.remove_punctuation(text)
        text = self.remove_stop_words(text)
        return text

    def wordcount(self, doc):
        word_count = defaultdict(int)
        for sentence in doc:
            for word in sentence.split():
                word_count[word] += 1
        word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))
        return word_count

    def zipf_plot(self, doc):
        word_count = self.wordcount(doc)
        plt.figure(figsize=(50, 10))
        plt.tight_layout()
        plt.xlabel("Words", fontsize=27)
        plt.ylabel("Frequency", fontsize=27)
        plt.xticks(rotation=90, fontsize=5)
        plt.bar(word_count.keys(), word_count.values())

    def heaps_plot(self, doc):
        frequency = []
        for i in doc:
            f = len(i)
            l = []
            c = 0
            for j in i:
                if j not in l:
                    c += 1
                    l.append(j)
            frequency.append([f, c])
        frequency.sort(reverse=True)
        x, y = [t[0] for t in frequency], [t[1] for t in frequency]
        plt.plot(x, y)

    def cosine_sim(self, query):
        q_tfidf = self.vectorizer.transform([query])
        results = cosine_similarity(self.X, q_tfidf).reshape((-1,))
        return results

    def query_search(self, results):
        print("Top 10 Documents matched: ")
        j = 0
        res = []
        for i in results.argsort()[-10:][::-1]:
            j += 1
            print("Document {:02d}: ".format(j), self.documents[i])
            res.append(self.documents[i])
        return res

    def get_relevance(self, results):
        res_bool = []
        for x in range(len(results)):
            ch = input(
                f"Type R or N for document {x+1} being relevant and non-relevant respectively: "
            )
            if ch == "R" or ch == "r":
                res_bool.append(True)
            else:
                res_bool.append(False)
        return res_bool

    def get_vectors(self, docs):
        vec = []
        for x in docs:
            i = self.documents.index(x)
            vec.append(self.X[i])
        return vec

    def rocchio_rel(self, query, vectors, rel_vec, alpha=1, beta=0.8, gamma=0.1):
        rels_ = [vectors[i] for i in range(len(vectors)) if rel_vec[i] == True]
        non_rels = [vectors[i] for i in range(len(vectors)) if rel_vec[i] == False]
        C1 = sum(rels_) / len(rels_)
        C2 = sum(non_rels) / len(non_rels)
        q = self.vectorizer.transform([query])
        q1 = alpha * q + beta * C1 - gamma * C2
        results = cosine_similarity(self.X, q1).reshape((-1,))
        return results

    def selected_topics(self, model, vectorizer, top_n=10):
        for idx, topic in enumerate(model.components_):
            print("Topic %d:" % (idx))
            print(
                [
                    (vectorizer.get_feature_names_out()[i], topic[i])
                    for i in topic.argsort()[: -top_n - 1 : -1]
                ]
            )

    def run(self):
        query = "neural machine learning"
        query_docs = self.query_search(self.cosine_sim(query))

        relevance = self.get_relevance(query_docs)
        tfidf_vectors = self.get_vectors(query_docs)
        rocchio_results = self.rocchio_rel(query, tfidf_vectors, relevance)
        q_docs = self.query_search(rocchio_results)

        self.selected_topics(self.lda, self.countvect)


analyzer = TextAnalyzer("/content/keywords.txt", "/content/documents(1).txt")
analyzer.run()
