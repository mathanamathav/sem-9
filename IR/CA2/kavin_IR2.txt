from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer,  CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import math
import string
from itertools import groupby
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

def remove_stop_words(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)
    return text

def lowercase(text):
  return text.lower()

def preprocess(text):
  text = lowercase(text)
  text = remove_punctuation(text)
  text = remove_stop_words(text)
  return text

keywords = open("/content/keywords (1).txt").readlines()
keywords = [x.strip() for x in keywords]

documents = open("/content/documents (1).txt", encoding='utf-8-sig').read().split('\n\n')
documents = [x.replace('\n', ' ').replace('...', '').replace(' ', '').replace('.  ', '.') for x in documents]

process_docs = [preprocess(x).lower() for x in documents]

def wordcount(doc):
  word_count = defaultdict(int)
  for sentence in doc:
    for word in sentence.split():
      word_count[word] += 1
  word_count = dict(sorted(word_count.items(), key = lambda x : x[1], reverse = True))
  return word_count

def zipf_plot(doc):
  word_count = wordcount(doc)
  plt.figure(figsize = (50, 10))
  plt.tight_layout()
  plt.xlabel('Words', fontsize = 27)
  plt.ylabel('Frequency', fontsize = 27)
  plt.xticks(rotation=90, fontsize = 5)
  plt.bar(word_count.keys(), word_count.values())

zipf_plot(process_docs)

def heaps_plot(doc):
    frequency = []
    for i in doc:
        f = len(i)
        l = []
        c = 0
        for j in i:
          if j not in l:
            c+=1
            l.append(j)
        frequency.append([f, c])
    frequency.sort(reverse = True)
    x, y = [t[0] for t in frequency], [t[1] for t in frequency]
    plt.plot(x,y)

heaps_plot(process_docs)

vectorizer = TfidfVectorizer(vocabulary=keywords)
X = vectorizer.fit_transform(process_docs)
print(X.shape)

def cosine_sim(query, X):
    q_tfidf = vectorizer.fit_transform([query])
    results = cosine_similarity(X,q_tfidf).reshape((-1,))
    return results

def query_search(results):
    print("Top 10 Documents matched: ")
    j = 0
    res = []
    for i in results.argsort()[-10:][::-1]:
        j += 1
        print("Document {:02d}: ".format(j), documents[i])
        res.append(documents[i])
    return res

def get_relevance(results):
    res_bool = []
    for x in range(len(results)):
        ch = input(f"Type R or N for document {x+1} being relevant and non-relevant respectively: ")
        if ch == 'R' or ch == 'r':
            res_bool.append(True)
        else:
            res_bool.append(False)
    return res_bool

def get_vectors(docs):
    vec = []
    for x in docs:
        i = documents.index(x)
        vec.append(X[i])
    return vec

def rocchio_rel(query, vectors, rel_vec, alpha=1, beta=0.8, gamma=0.1):
    rels_ = [vectors[i] for i in range(len(vectors)) if rel_vec[i] == True]
    non_rels = [vectors[i] for i in range(len(vectors)) if rel_vec[i] == False]
    C1 = sum(rels_) / len(rels_)
    C2 = sum(non_rels) / len(non_rels)
    q = vectorizer.fit_transform([query])
    q1 = alpha * q + beta * C1 - gamma * C2
    results = cosine_similarity(X,q1).reshape((-1,))
    return results

query = 'neural machine learning'
query_docs = query_search(cosine_sim(query, X))

relevance = get_relevance(query_docs)
tfidf_vectors = get_vectors(query_docs)
rocchio_results = rocchio_rel(query, tfidf_vectors, relevance)
q_docs = query_search(rocchio_results)

countvect = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\-][a-zA-Z\-]{2,}')
count_docs = countvect.fit_transform(process_docs)

NUM_TOPICS = 10
lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)
data_lda = lda.fit_transform(count_docs)

def selected_topics(model, vectorizer, top_n=10):
    for idx, topic in enumerate(model.components_):
        print("Topic %d:" % (idx))
        print([(vectorizer.get_feature_names_out()[i], topic[i])
                        for i in topic.argsort()[:-top_n - 1:-1]])

print('LDA Model: ')
selected_topics(lda, countvect)